# ============================================================================
# Ground Truth Evaluation Configuration
# ============================================================================
#
# Evaluates LLM inference performance against human ground truth.
# For detailed documentation, see: src/evaluate/PIPELINE_GUIDE.md
#
# Quick Reference:
#   Output: data/spia/gt_vs_llms/{gt}_vs_{llm}_{suffix}_evaluation.json
#   Scripts: align_human_llm_annotations → align_subject_based_ground_truth (x2) → check_human_labels_gt
# ============================================================================


input:
  # Ground truth file (human-annotated SPIA format)
  gt_file: "data/spia/spia_tab_144.jsonl"

  # LLM inference output file (generated by src/inference/infer_spia.py)
  llm_file: "output/inference/spia_tab_144_inferred.jsonl"

output:
  # Suffix for output files (recommendation: model_numDocs_date)
  suffix: "example_144"

align_subject_based_ground_truth:
  api_provider: "openai"
  model: "gpt-4.1-mini"
  processing_mode: "concurrent"
  num_workers: 20
  document_limit: null

check_human_labels_gt:
  api_provider: "openai"
  model: "gpt-4.1-mini"
  min_certainty: 3
  tags_to_evaluate: null
  location_max_depth: 4
  decider: "rule_model_human"
  num_workers: 20
  show_case_logs: true
  case_log_filter: "disagree"  # Options: all, disagree (score < 1.0), mismatch (score = 0.0)
